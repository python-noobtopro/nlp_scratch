{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286b7491",
   "metadata": {},
   "source": [
    "## Basics of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6508fc",
   "metadata": {},
   "source": [
    "#### The main motive of NLP is to convert words into numerical vectors so that we can do operations on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0062ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237d46b",
   "metadata": {},
   "source": [
    "#### 1. Bags of words Approach"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7f3e482",
   "metadata": {},
   "source": [
    "# This approach combines all the sentences (utterences) into one and treats them as a bag and does operation on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c033b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d6134b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "# Objective is to convert the text into numerical vector\n",
    "\n",
    "train_x = ['i love the book',\n",
    "          'this is a great book',\n",
    "          'the fit is great',\n",
    "          'i love the shoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02fc48be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book' 'fit' 'great' 'is' 'love' 'shoes' 'the' 'this']\n",
      "[[1 0 0 0 1 0 1 0]\n",
      " [1 0 1 1 0 0 0 1]\n",
      " [0 1 1 1 0 0 1 0]\n",
      " [0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiating\n",
    "# Unigram Model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Bigram model\n",
    "# vectorizer = CountVectorizer(binary=True, ngram=(1,2))\n",
    "\n",
    "# Training\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "# getting all feature names\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Numerical Vector Array of the text\n",
    "print(train_x_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bc357",
   "metadata": {},
   "source": [
    "###### One point to note that this vectorizer ignores single letter by default hence, a and i are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1b8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we are going forward to classify the data\n",
    "# Let's declare our train classifier\n",
    "\n",
    "class Category:\n",
    "    BOOKS = 'BOOKS'\n",
    "    CLOTHING = 'CLOTHING'\n",
    "    \n",
    "train_y = [Category.BOOKS, \n",
    "           Category.BOOKS, \n",
    "           Category.CLOTHING, \n",
    "           Category.CLOTHING]\n",
    "\n",
    "# Basically we have classified each train_x line to a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1db441dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on Category\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d5c3e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting on unknown values\n",
    "\n",
    "new_x = vectorizer.transform(['i like this book'])\n",
    "clf_svm.predict(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ddfdca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x = vectorizer.transform(['i like this shoes'])\n",
    "clf_svm.predict(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If it hasn't seen a word it will predict badly. That should be Noted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7688a",
   "metadata": {},
   "source": [
    "#### 2. Word Vectors using SpaCy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d76ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca934b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a778a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3563da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love the book', 'this is a great book', 'the fit is great', 'i love the shoes']\n"
     ]
    }
   ],
   "source": [
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aab15a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(text) for text in train_x]\n",
    "train_x_word_vectors = [x.vector for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e682e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[1].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace97ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_wv = svm.SVC(kernel='linear')\n",
    "clf_svm_wv.fit(train_x_word_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5554d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = ['this is a story of a king']\n",
    "test_docs = [nlp(text) for text in test_x]\n",
    "test_x_wv = [x.vector for x in test_docs]\n",
    "clf_svm_wv.predict(test_x_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using spacy library we can predict much more words\n",
    "## It is really powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c0999",
   "metadata": {},
   "source": [
    "#### Regexes for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb6c21f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcd', 'abghsgcd']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: start with 'ab'\n",
    "# no whitespaces\n",
    "# end with 'cd'\n",
    "\n",
    "import re\n",
    "regexp = re.compile(r'^ab[^\\s]*cd$')\n",
    "test = ['abcd', 'xx', 'ab cd', 'abghsgcd']\n",
    "\n",
    "match=[]\n",
    "for i in test:\n",
    "    if re.match(regexp, i):\n",
    "        match.append(i)\n",
    "        \n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabf8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If we want to search whether our regex is in the word we can use re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for words in phrases\n",
    "# re.compile('\\bread|\\bstory|\\bbook')\n",
    "# \\b word boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf9ff6",
   "metadata": {},
   "source": [
    "#### 3. Stemming/Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f8628c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b1f5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f363e4",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0f6a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'read', 'a', 'book', 'and', 'write']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i am read a book and write'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes sentence and breaks it into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Stems words (normalizes)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "phrase = 'I am reading a book and writing'\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words:\n",
    "    stemmed_words.append(stemmer.stem(word))\n",
    "print(stemmed_words)\n",
    "\n",
    "\" \".join(stemmed_words)\n",
    "\n",
    "# You should avoid punctuations in phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cb53c",
   "metadata": {},
   "source": [
    "##### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61962b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'read', 'a', 'book', 'and', 'write']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I be read a book and write'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "phrase = 'I am reading a book and writing'\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "lemmatized_words = []\n",
    "for word in words:\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "print(lemmatized_words)\n",
    "\n",
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fbd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reducing verbs back to base word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9c3a2",
   "metadata": {},
   "source": [
    "#### 4. Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9baa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## They are the common words im English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb33e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1af95ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'going', 'market', 'buy', 'clothes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I going market buy clothes'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "\n",
    "phrase = 'I am going to the market to buy some clothes'\n",
    "\n",
    "words = word_tokenize(phrase)\n",
    "\n",
    "stripped_words = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stripped_words.append(word)\n",
    "print(stripped_words)\n",
    "\n",
    "\" \".join(stripped_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65036193",
   "metadata": {},
   "source": [
    "##### Some other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8007f",
   "metadata": {},
   "source": [
    "##### Spell Correction, Sentiment, Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c64e8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c486e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a bad example\n"
     ]
    }
   ],
   "source": [
    "# Spell Correction\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "phrase = 'this is a bad exampelee'\n",
    "\n",
    "tb_phrase = TextBlob(phrase)\n",
    "print(tb_phrase.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf3de192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b185c563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('bad', 'JJ'),\n",
       " ('exampelee', 'NN')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_phrase.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN means Noun, VBZ means Verb, DT is determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3221e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_phrase.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative polarity due to use of 'bad'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
